{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>আমাদের সজাগ থাকতে হবে টিকা নেওয়া নিয়ে।</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ভ্যাকসিন ভালো না।</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>আমার পরিবারের সকলেই টিকা নিয়েছে।</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>টিকা লক্ষণীয় রোগ প্রতিরোধে একইভাবে উচ্চ কার্য...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>অনলাইনে আবেদন করে আমি ভ্যাকসিন নিয়েছি।</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Polarity\n",
       "0            আমাদের সজাগ থাকতে হবে টিকা নেওয়া নিয়ে।          1\n",
       "1                                  ভ্যাকসিন ভালো না।         0\n",
       "2                 আমার পরিবারের সকলেই টিকা নিয়েছে।          1\n",
       "3  টিকা লক্ষণীয় রোগ প্রতিরোধে একইভাবে উচ্চ কার্য...         1\n",
       "4           অনলাইনে আবেদন করে আমি ভ্যাকসিন নিয়েছি।          1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>কেন্দ্রে প্রচণ্ড ভিড় থাকায় অনেকে ভেক্সিন পায...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3182</th>\n",
       "      <td>ভ্যাকসিন দিতে গিয়ে কোনরকম ঝামেলার সম্মুখীন হত...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3350</th>\n",
       "      <td>করোনার ভ্যাকসিনগুলি আপনাকে করোনায় অসুস্থ করতে ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>ভ্যাকসিন নিয়ে আমি ভালো আছি।</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2293</th>\n",
       "      <td>টিকা ছাড়াও ভালো হয়।</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  Polarity\n",
       "1011  কেন্দ্রে প্রচণ্ড ভিড় থাকায় অনেকে ভেক্সিন পায...         0\n",
       "3182  ভ্যাকসিন দিতে গিয়ে কোনরকম ঝামেলার সম্মুখীন হত...         1\n",
       "3350  করোনার ভ্যাকসিনগুলি আপনাকে করোনায় অসুস্থ করতে ...         1\n",
       "897                        ভ্যাকসিন নিয়ে আমি ভালো আছি।         1\n",
       "2293                               টিকা ছাড়াও ভালো হয়।         0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_shuffle = df.sample(frac = 1,random_state=42)\n",
    "df_shuffle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2007\n",
       "0    1800\n",
       "Name: Polarity, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Polarity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentence,test_sentence,train_label,test_label = train_test_split(df_shuffle['Text'].to_numpy(),df_shuffle['Polarity'].to_numpy(),test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['সময় মত ২য় ডোজ নিতে দেরি করলে মানুষ মারা যায়।',\n",
       "       'টিকা নিতে হবে। টিকে থাকতে হবে এই মহামারীতে।',\n",
       "       'টিকা নেওয়ার পর আমার জ্বর এসেছিলো। আমি মনে করি যে টিকাটি আমার শরীরের ঠিক কাজ করেছে।',\n",
       "       'কিছু ধরণের COVID-19 ভ্যাকসিন মেসেঞ্জার RNA (mRNA) ব্যবহার করে তৈরি করা হয়েছে, এটি একটি নতুন প্রযুক্তি যা প্রচলিত পদ্ধতির ভ্যাকসিন তৈরির চেয়ে দ্রুত পদ্ধতির অনুমতি দেয়।',\n",
       "       'আপনার কাছে যখন বিকল্প উপায়ে থাকে, তখন ভ্যাকসিন নেওয়ার কোনো প্রয়োজন নেই।',\n",
       "       'ভেকসিনই একমাত্র উপায় নয় আরও অনেক উপায় আছে।',\n",
       "       'সরকারের পক্ষ থেকে ইউনিয়নভিত্তিক টিকা প্রদানের যে পরিকল্পনা গ্রহণ করা হয়েছে সেখানে অনেক দুর্নীতি হচ্ছে। ',\n",
       "       'টিকার জন্য নিবন্ধন করেছি কিন্তু আমার এসএমএস আসছে না',\n",
       "       'আমি বিদেশ যাবো দেখে আগে আগেই টিকা নিয়ে নিছি। ',\n",
       "       'টিকা গ্রহণের পর দুর্বল হয়ে যায় বা টিকা গ্রহণের পর লোকে মারা যায়, গ্রাম ও প্রত্যন্ত অঞ্চলে এই ধরণের লোকেদের ভুল বিশ্বাস ভাঙা উচিত বলেও জানিয়েছেন।'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentence[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target:0 (Negative FeedBack)\n",
      "Text\n",
      " ভ্যাকসিন নিলে তো অনেক সমস্যা দেখা দেয়।\n",
      "\n",
      "---\n",
      "\n",
      "Target:0 (Negative FeedBack)\n",
      "Text\n",
      " আর কেউ খাচ্ছে না টিকা, তাই বাঙালি কে খাওয়া এখন।\n",
      "\n",
      "---\n",
      "\n",
      "Target:0 (Negative FeedBack)\n",
      "Text\n",
      " ভেকসিন এ গরুর রক্ত থাকে।\n",
      "\n",
      "---\n",
      "\n",
      "Target:0 (Negative FeedBack)\n",
      "Text\n",
      " কোভিড ভ্যাকসিন সম্পূর্ণ হালাল কিনা তা এখনও গবেষণা করে পাওয়া যায়নি। \n",
      "\n",
      "---\n",
      "\n",
      "Target:1 (Positive FeedBack)\n",
      "Text\n",
      " কেউ যদি প্রথম ডোজ নিয়ে থাকে এবং তার যদি কিছু উপসর্গ দেখা না দেয়, তাহলে নির্দ্বিধায়-নির্ভয়ে দ্বিতীয় ডোজ নিতে হবে।\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# visualize some random training examples\n",
    "\n",
    "import random\n",
    "\n",
    "random_index = random.randint(0,len(train_sentence) -5)\n",
    "\n",
    "for row in df_shuffle[['Text','Polarity']][random_index:random_index+5].itertuples():\n",
    "    _,text,polarity = row\n",
    "    print(f\"Target:{polarity}\", \"(Positive FeedBack)\" if polarity > 0 else \"(Negative FeedBack)\")\n",
    "    print(f\"Text\\n {text}\\n\")\n",
    "    print(\"---\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3045, 3045, 762, 762)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(train_sentence), len(train_label),len(test_sentence),len(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-19 21:43:04.634507: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-19 21:43:05.623478: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-19 21:43:05.624804: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-19 21:43:09.410005: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-19 21:43:09.410843: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-19 21:43:09.410883: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-02-19 21:43:13.143982: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-02-19 21:43:13.144833: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (GalibPc): /proc/driver/nvidia/version does not exist\n",
      "2023-02-19 21:43:13.146074: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers.preprocessing.text_vectorization import TextVectorization\n",
    "import numpy as np\n",
    "\n",
    "text_vec = TextVectorization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_word_per_sentence = round(sum(len(i.split()) for i in train_sentence)/len(train_sentence))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings Parmaeters for TextVectorization\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_length = 10000\n",
    "max_length = average_word_per_sentence\n",
    "text_vectorized = TextVectorization(\n",
    "    max_tokens=max_vocab_length,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorized.adapt(train_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 11), dtype=int64, numpy=array([[152, 498, 522,  10,  14, 672, 569,  33, 175, 189,   0]])>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sentence = 'সময় মত ২য় ডোজ নিতে দেরি করলে মানুষ মারা যায়।'\n",
    "\n",
    "text_vectorized([sample_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "\n",
      "করোনাকে ভয় না পেয়ে সচেতন নাগরিক হিসেবে মাক্স ব্যবহার করুন, ভেক্সিন নিতে হবে না।\n",
      "\n",
      " Vectorized version : [[332  89   6 840 319 553 180 728 270 416  39]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "random_sentence = random.choice(train_sentence)\n",
    "print(f\"Original text:\\n\\n{random_sentence}\\n\\n Vectorized version : {text_vectorized([random_sentence])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocabulary 5201\n",
      "\n",
      "Top 10 words : ['', '[UNK]', 'টিকা', 'ভ্যাকসিন', 'আমি', 'না।', 'না', 'করোনা', 'জন্য', 'এবং']\n",
      "Bottom 10 words : ['c130j', 'astrazenecaএর', '95।', '95', '90', '6', '3', '25', '18', '10']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "word_to_vocab = text_vectorized.get_vocabulary()\n",
    "\n",
    "top_10_word = word_to_vocab[:10]\n",
    "bottom_10_word = word_to_vocab[-10:]\n",
    "\n",
    "print(f\"Number of words in vocabulary {len(word_to_vocab)}\\n\")\n",
    "print(f\"Top 10 words : {top_10_word}\")\n",
    "print(f\"Bottom 10 words : {bottom_10_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.embedding.Embedding at 0x7f33cee836d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# making an embedding layer\n",
    "\n",
    "from keras.layers import Embedding\n",
    "\n",
    "embedding = Embedding(\n",
    "    input_dim=max_vocab_length,\n",
    "    output_dim=128,\n",
    "    input_length=max_length,\n",
    "   \n",
    ")\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original Text :\n",
      "\n",
      " আমি মনে করি টিকা নেওয়া জরুরী এবং উচিত। \n",
      "\n",
      " Embedding Version : \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 11, 128), dtype=float32, numpy=\n",
       "array([[[ 0.04211397,  0.04096452, -0.03018483, ...,  0.02016247,\n",
       "          0.00776647,  0.00674547],\n",
       "        [ 0.01861342,  0.02863599,  0.00116309, ..., -0.0296996 ,\n",
       "          0.04511601, -0.02943499],\n",
       "        [ 0.01278445,  0.01401693, -0.00447685, ...,  0.01015432,\n",
       "          0.01870907,  0.00307056],\n",
       "        ...,\n",
       "        [ 0.04439985,  0.04656483,  0.00499258, ...,  0.01102858,\n",
       "          0.04520801, -0.02584496],\n",
       "        [ 0.04439985,  0.04656483,  0.00499258, ...,  0.01102858,\n",
       "          0.04520801, -0.02584496],\n",
       "        [ 0.04439985,  0.04656483,  0.00499258, ...,  0.01102858,\n",
       "          0.04520801, -0.02584496]]], dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_sentence = random.choice(train_sentence)\n",
    "\n",
    "print(f\" Original Text :\\n\\n {random_sentence} \\n\\n Embedding Version : \")\n",
    "sample_emb = embedding(text_vectorized([random_sentence])) \n",
    "sample_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
       "array([ 0.04211397,  0.04096452, -0.03018483, -0.00186721,  0.04599554,\n",
       "        0.02457141,  0.02337706,  0.01491535,  0.00303651,  0.04636452,\n",
       "        0.03826341,  0.00122217, -0.01218275,  0.00072576,  0.0346709 ,\n",
       "        0.0048288 ,  0.00942798,  0.02333397, -0.01163299, -0.02142945,\n",
       "       -0.00664592,  0.01013374,  0.00406834,  0.04113383, -0.01086297,\n",
       "        0.01723566,  0.03294731,  0.0207161 , -0.0263526 ,  0.03751827,\n",
       "        0.02187902, -0.03216145,  0.02355563, -0.04447166, -0.04998688,\n",
       "        0.01458291,  0.02732556, -0.03208004, -0.00356527,  0.02390702,\n",
       "        0.03664713, -0.0189574 , -0.018169  ,  0.02323525, -0.02777487,\n",
       "       -0.00546936, -0.01256884,  0.0128341 , -0.00848389,  0.02096841,\n",
       "       -0.01326156,  0.01538217,  0.01885146,  0.0255036 , -0.03469086,\n",
       "        0.04456017, -0.04695258, -0.02769562,  0.04253255, -0.02338432,\n",
       "        0.03765735, -0.04828607,  0.04138282, -0.01866494, -0.01895246,\n",
       "        0.01530961,  0.04995597,  0.0280314 , -0.0055575 ,  0.00648819,\n",
       "       -0.03668584,  0.03589583, -0.01319289,  0.01770734,  0.04414039,\n",
       "       -0.03607766, -0.02497449, -0.00725514, -0.02357498,  0.045268  ,\n",
       "        0.03798551, -0.00579916,  0.02567244, -0.04905307, -0.03497478,\n",
       "       -0.03827084,  0.02483037, -0.03333763, -0.04444891,  0.03216274,\n",
       "        0.04809893, -0.03202078,  0.0182887 , -0.00854653, -0.0173548 ,\n",
       "        0.03843651, -0.00500761, -0.02373424, -0.02800362, -0.04070926,\n",
       "        0.00611861,  0.01775335,  0.01843244,  0.02538748,  0.04072322,\n",
       "        0.04976498, -0.02134316, -0.0323166 , -0.00352956,  0.00875577,\n",
       "        0.03444313,  0.04550424,  0.04819827,  0.04312284,  0.04830648,\n",
       "        0.01170022,  0.02035714, -0.02004796, -0.02329062, -0.00549837,\n",
       "       -0.04958736, -0.04885485, -0.0308045 ,  0.010393  ,  0.04435298,\n",
       "        0.02016247,  0.00776647,  0.00674547], dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_emb[0][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 : Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;multiNB&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;multiNB&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('multiNB', MultinomialNB())])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "model_naive_bayes = Pipeline([\n",
    "    ('tfidf',TfidfVectorizer()),\n",
    "    ('multiNB',MultinomialNB())\n",
    "])\n",
    "\n",
    "model_naive_bayes.fit(train_sentence,train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Naive Bayes accuracy: 66.80 %\n"
     ]
    }
   ],
   "source": [
    "score = model_naive_bayes.score(test_sentence,test_label)\n",
    "print(f'Our Naive Bayes accuracy: {score*100:.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, 1, 0]), array([0, 0, 1, 0, 0]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Prediction Test\n",
    "\n",
    "naive_bayes_pred = model_naive_bayes.predict(test_sentence[:5])\n",
    "\n",
    "naive_bayes_pred[:5], test_label[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0]), 0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_naive_bayes.predict([test_sentence[1]]),test_label[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: এত টীকা আসতেছে তাহলে গণ টীকা কার্যক্রম গতিহীন কেন? তবে কি দূর্নীতিবাজরা আবার টীকা বিক্রির পায়তারা করছে?  pred: 1 original: 1\n",
      "\n",
      "\n",
      "Text: এইবার ছুটিতে বাড়ি গিয়ে আমি টিকা নিব।  pred: 1 original: 1\n",
      "\n",
      "\n",
      "Text: কোভিডের টিকা আমি তিনটাই সম্পন্ন করেছি।  pred: 1 original: 0\n",
      "\n",
      "\n",
      "Text: আমিি যুবক মানুষ আমার টিকা লাগে না।  pred: 1 original: 0\n",
      "\n",
      "\n",
      "Text: আমার অনেক সমস্যা আছে আমি ভ্যাকসিন নিব না।  pred: 0 original: 1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdint = random.randint(0,abs(len(test_sentence) -5))\n",
    "\n",
    "for i,word in enumerate(test_sentence[rdint:rdint+5]):\n",
    "  \n",
    "    print(f\"Text: {word}  pred: {model_naive_bayes.predict([word])[0]} original: {test_label[rdint+i+1]}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Accuracy\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def calculate_results(y_true,y_pred):\n",
    "    model_accuracy = accuracy_score(y_true,y_pred) *100\n",
    "    precision,recall,f1,_ = precision_recall_fscore_support(y_true,y_pred,average='weighted')\n",
    "    model_result = {\n",
    "        'accuracy':model_accuracy,\n",
    "        'precision':precision,\n",
    "        'recall':recall,\n",
    "        'f1':f1\n",
    "    }\n",
    "    return model_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 66.7979002624672,\n",
       " 'precision': 0.6960798636606401,\n",
       " 'recall': 0.6679790026246719,\n",
       " 'f1': 0.6729967926241313}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_results(model_naive_bayes.predict(test_sentence),test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open ('models/naive_bayes.pkl', 'wb') as f:\n",
    "    pickle.dump(model_naive_bayes,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model( algorithm, train_sentence=train_sentence, train_label=train_label):\n",
    "    model_pipeline = Pipeline([\n",
    "    ('tfidf',TfidfVectorizer()),\n",
    "    ('Algorithum',algorithm)\n",
    "    ])\n",
    "    model_pipeline.fit(train_sentence,train_label)\n",
    "    return model_pipeline\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic   Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 68.11023622047244,\n",
       " 'precision': 0.6878497554576837,\n",
       " 'recall': 0.6811023622047244,\n",
       " 'f1': 0.6823122073541078}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model_logistic_regression = build_model(LogisticRegression())\n",
    "model_logistic_regression.score(test_sentence,test_label)\n",
    "calculate_results(model_logistic_regression.predict(test_sentence),test_label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model with Random Forest, Gradient Boosting, XGBoost, LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 66.14173228346458,\n",
       " 'precision': 0.6624484936730389,\n",
       " 'recall': 0.6614173228346457,\n",
       " 'f1': 0.6616414616391161}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC   \n",
    "model_linear_svc = build_model(LinearSVC())\n",
    "model_linear_svc.score(test_sentence,test_label)\n",
    "calculate_results(model_linear_svc.predict(test_sentence),test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 70.86614173228347,\n",
       " 'precision': 0.7099220627322717,\n",
       " 'recall': 0.7086614173228346,\n",
       " 'f1': 0.7088965103912331}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_random_forest = build_model(RandomForestClassifier())\n",
    "model_random_forest.score(test_sentence,test_label)\n",
    "calculate_results(model_random_forest.predict(test_sentence),test_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 69.02887139107612,\n",
       " 'precision': 0.708761254131876,\n",
       " 'recall': 0.6902887139107612,\n",
       " 'f1': 0.6933952975370142}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model_gradient_boosting = build_model(GradientBoostingClassifier())\n",
    "model_gradient_boosting.score(test_sentence,test_label)\n",
    "calculate_results(model_gradient_boosting.predict(test_sentence),test_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 67.45406824146981,\n",
       " 'precision': 0.6753414383607161,\n",
       " 'recall': 0.6745406824146981,\n",
       " 'f1': 0.6747134648709452}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "model_adaboost = build_model(AdaBoostClassifier())\n",
    "model_adaboost.score(test_sentence,test_label)\n",
    "calculate_results(model_adaboost.predict(test_sentence),test_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 71.5223097112861,\n",
       " 'precision': 0.7166157943784878,\n",
       " 'recall': 0.7152230971128609,\n",
       " 'f1': 0.7154750142924149}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "model_extra_tree = build_model(ExtraTreesClassifier())\n",
    "model_extra_tree.score(test_sentence,test_label)\n",
    "calculate_results(model_extra_tree.predict(test_sentence),test_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 66.01049868766404,\n",
       " 'precision': 0.6609413032437741,\n",
       " 'recall': 0.6601049868766404,\n",
       " 'f1': 0.6601945618874624}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "model_bagging = build_model(BaggingClassifier())\n",
    "model_bagging.score(test_sentence,test_label)\n",
    "calculate_results(model_bagging.predict(test_sentence),test_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 56.03674540682415,\n",
       " 'precision': 0.7169998975592242,\n",
       " 'recall': 0.5603674540682415,\n",
       " 'f1': 0.5967490612916128}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model_knn = build_model(KNeighborsClassifier())\n",
    "model_knn.score(test_sentence,test_label)\n",
    "calculate_results(model_knn.predict(test_sentence),test_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 63.25459317585301,\n",
       " 'precision': 0.6325459317585301,\n",
       " 'recall': 0.6325459317585301,\n",
       " 'f1': 0.6325459317585301}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model_decision_tree = build_model(DecisionTreeClassifier())\n",
    "model_decision_tree.score(test_sentence,test_label)\n",
    "calculate_results(model_decision_tree.predict(test_sentence),test_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#xgboost\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mxgboost\u001b[39;00m \u001b[39mimport\u001b[39;00m XGBClassifier\n\u001b[1;32m      3\u001b[0m model_xgboost \u001b[39m=\u001b[39m build_model(XGBClassifier())\n\u001b[1;32m      4\u001b[0m model_xgboost\u001b[39m.\u001b[39mscore(test_sentence,test_label)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "#xgboost\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model_xgboost = build_model(XGBClassifier())\n",
    "model_xgboost.score(test_sentence,test_label)\n",
    "calculate_results(model_xgboost.predict(test_sentence),test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a function to save model\n",
    "\n",
    "def save_model(model,filename):\n",
    "    with open(f'models/{filename}.pkl','wb') as f:\n",
    "        pickle.dump(model,f)\n",
    "\n",
    "save_model(model_naive_bayes,'naive_bayes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# make a function to visualize the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvisualize_model\u001b[39m(model, model_name):\n\u001b[1;32m      5\u001b[0m     fig, ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(figsize\u001b[39m=\u001b[39m(\u001b[39m10\u001b[39m, \u001b[39m10\u001b[39m))\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "\n",
    "# make a function to visualize the model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_model(model, model_name):\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    plot_confusion_matrix(model, test_sentence, test_label, ax=ax, cmap=plt.cm.Blues)\n",
    "    plt.title(f\"{model_name} Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "visualize_model(model_naive_bayes, 'Naive Bayes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BuildModel():\n",
    "\n",
    "    def __init__(self,train_sentence,train_label,test_sentence,test_label,algo) -> None:\n",
    "        self.train_sentence = train_sentence\n",
    "        self.train_label = train_label\n",
    "        self.test_sentence = test_sentence\n",
    "        self.test_label = test_label\n",
    "        self.algo = algo\n",
    "\n",
    "    \n",
    "    def run(self):\n",
    "        self.build_model()\n",
    "        self.score()\n",
    "        return self.calculate_results()\n",
    "\n",
    "    def build_model(self):\n",
    "        self.model = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer()),\n",
    "            ('algo',algo)\n",
    "        ])\n",
    "        return self.model.fit(self.train_sentence,train_label)\n",
    "    \n",
    "    def score(self):\n",
    "        return self.model.score(test_sentence,test_label)\n",
    "\n",
    "    def calculate_results(self):\n",
    "        y_true = test_label\n",
    "        y_pred = self.model.predict(test_sentence)\n",
    "        model_accuracy = accuracy_score(y_true,y_pred) *100\n",
    "        precision,recall,f1,_ = precision_recall_fscore_support(y_true,y_pred,average='weighted')\n",
    "        model_result = {\n",
    "            'accuracy':model_accuracy,\n",
    "            'precision':precision,\n",
    "            'recall':recall,\n",
    "            'f1':f1\n",
    "        }\n",
    "        return model_result\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'algo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m naive_bayes_model_run \u001b[39m=\u001b[39m BuildModel(train_sentence,train_label,test_sentence,test_label,algo\u001b[39m=\u001b[39mMultinomialNB())\n\u001b[0;32m----> 2\u001b[0m naive_bayes_model_run\u001b[39m.\u001b[39;49mrun()\n",
      "Cell \u001b[0;32mIn[28], line 12\u001b[0m, in \u001b[0;36mBuildModel.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 12\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuild_model()\n\u001b[1;32m     13\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscore()\n\u001b[1;32m     14\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcalculate_results()\n",
      "Cell \u001b[0;32mIn[28], line 19\u001b[0m, in \u001b[0;36mBuildModel.build_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild_model\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     17\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m Pipeline([\n\u001b[1;32m     18\u001b[0m         (\u001b[39m'\u001b[39m\u001b[39mtfidf\u001b[39m\u001b[39m'\u001b[39m, TfidfVectorizer()),\n\u001b[0;32m---> 19\u001b[0m         (\u001b[39m'\u001b[39m\u001b[39malgo\u001b[39m\u001b[39m'\u001b[39m,algo)\n\u001b[1;32m     20\u001b[0m     ])\n\u001b[1;32m     21\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mfit(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_sentence,train_label)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'algo' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# naive_bayes_model_run = BuildModel(train_sentence,train_label,test_sentence,test_label,algo=MultinomialNB())\n",
    "# naive_bayes_model_run.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'algo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mensemble\u001b[39;00m \u001b[39mimport\u001b[39;00m RandomForestClassifier\n\u001b[1;32m      3\u001b[0m svc \u001b[39m=\u001b[39m BuildModel(train_sentence,train_label,test_sentence,test_label,algo\u001b[39m=\u001b[39mRandomForestClassifier())\n\u001b[0;32m----> 4\u001b[0m svc\u001b[39m.\u001b[39;49mrun()\n",
      "Cell \u001b[0;32mIn[28], line 12\u001b[0m, in \u001b[0;36mBuildModel.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 12\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuild_model()\n\u001b[1;32m     13\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscore()\n\u001b[1;32m     14\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcalculate_results()\n",
      "Cell \u001b[0;32mIn[28], line 19\u001b[0m, in \u001b[0;36mBuildModel.build_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild_model\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     17\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m Pipeline([\n\u001b[1;32m     18\u001b[0m         (\u001b[39m'\u001b[39m\u001b[39mtfidf\u001b[39m\u001b[39m'\u001b[39m, TfidfVectorizer()),\n\u001b[0;32m---> 19\u001b[0m         (\u001b[39m'\u001b[39m\u001b[39malgo\u001b[39m\u001b[39m'\u001b[39m,algo)\n\u001b[1;32m     20\u001b[0m     ])\n\u001b[1;32m     21\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mfit(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_sentence,train_label)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'algo' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "svc = BuildModel(train_sentence,train_label,test_sentence,test_label,algo=RandomForestClassifier())\n",
    "svc.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Neural Network \n",
    "import keras as ks\n",
    "from keras import layers,Model\n",
    "\n",
    "input = layers.Input(shape=(1,), dtype=\"string\")\n",
    "x = text_vectorized(input)\n",
    "x = embedding(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "nn_model = Model(input,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model.compile(loss='binary_crossentropy',optimizer=\"Adam\",metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 11)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 11, 128)           1280000   \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 128)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,280,129\n",
      "Trainable params: 1,280,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "96/96 [==============================] - 4s 29ms/step - loss: 0.6708 - accuracy: 0.6851\n",
      "Epoch 2/5\n",
      "96/96 [==============================] - 3s 28ms/step - loss: 0.5660 - accuracy: 0.8115\n",
      "Epoch 3/5\n",
      "96/96 [==============================] - 3s 28ms/step - loss: 0.4464 - accuracy: 0.8496\n",
      "Epoch 4/5\n",
      "96/96 [==============================] - 3s 28ms/step - loss: 0.3609 - accuracy: 0.8729\n",
      "Epoch 5/5\n",
      "96/96 [==============================] - 3s 27ms/step - loss: 0.3019 - accuracy: 0.8943\n"
     ]
    }
   ],
   "source": [
    "nn_history = nn_model.fit(train_sentence,train_label, epochs=5, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nn_pred = tf.squeeze(tf.round(nn_model.predict(test_sentence))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 77.42782152230971,\n",
       " 'precision': 0.7768009181492118,\n",
       " 'recall': 0.7742782152230971,\n",
       " 'f1': 0.7735962055713774}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_results(test_label,nn_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/nn_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/nn_model/assets\n"
     ]
    }
   ],
   "source": [
    "nn_model.save('models/nn_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d307a27cf840f16ae9578e3d15edb9dd96b6bcee180de4bfadc2593815b392e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
